{"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOx4oeMC3hTvxLyI9ZXkFLu","include_colab_link":true,"mount_file_id":"1yWAxW5Pe7WREzi42KucyDsCr7WO1nAQE","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"}},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7896990,"sourceType":"datasetVersion","datasetId":4637342}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/MnCSSJ4x/VR-MiniProject/blob/main/VR3b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"colab_type":"text","id":"view-in-github"}},{"cell_type":"code","source":"import torch \nfrom torch import optim, cuda\nimport torch.nn as nn\nimport torchvision\nfrom torchvision import transforms, datasets, models\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n# from torchsummary import summary\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport math \nfrom torch.utils.data import Dataset, TensorDataset, random_split\nfrom torchvision import transforms\nfrom tqdm import tqdm \nimport tarfile\n!pip install imutils\n\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport argparse \nimport joblib\nimport cv2\nimport os\nimport time \nfrom imutils import paths\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import models\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelBinarizer\n# from tqdm import tqdm_notebook as tqdm\nfrom tqdm import tqdm","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3L-gKiFhfaQd","outputId":"5f032a18-359e-4001-d83b-6846ea9a3816","execution":{"iopub.status.busy":"2024-03-20T15:42:48.731153Z","iopub.execute_input":"2024-03-20T15:42:48.731815Z","iopub.status.idle":"2024-03-20T15:43:04.933055Z","shell.execute_reply.started":"2024-03-20T15:42:48.731775Z","shell.execute_reply":"2024-03-20T15:43:04.931275Z"},"trusted":true},"execution_count":88,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: imutils in /opt/conda/lib/python3.10/site-packages (0.5.4)\n","output_type":"stream"}]},{"cell_type":"code","source":"cuda = cuda.is_available()\nprint(f'Train on gpu: {cuda}')\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Id5jfouGfd1X","outputId":"d046b858-6f88-46cf-b04a-5c51a7f5dab7","execution":{"iopub.status.busy":"2024-03-20T15:43:04.937141Z","iopub.execute_input":"2024-03-20T15:43:04.937753Z","iopub.status.idle":"2024-03-20T15:43:04.945995Z","shell.execute_reply.started":"2024-03-20T15:43:04.937696Z","shell.execute_reply":"2024-03-20T15:43:04.944731Z"},"trusted":true},"execution_count":89,"outputs":[{"name":"stdout","text":"Train on gpu: False\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Dataset Used \nThe dataset we are planning to use is the caltech 101 dataset.  It consist of about 40 to 800 images per category. Most categories have about 50 images. Collected in September 2003 by Fei-Fei Li, Marco Andreetto, and Marc'Aurelio Ranzato. The size of each image is roughly 300 x 200 pixels. ","metadata":{"id":"xHoO2d5ChGtu"}},{"cell_type":"code","source":"# image_paths = list(paths.list_images('/kaggle/input/multi-class-weather-dataset/Multi-class Weather Dataset'))\n# dataset = ImageFolder(root='/kaggle/input/multiclass-weather-dataset/Multi-class Weather Dataset', transform=transform)\nimage_paths = list(paths.list_images('/kaggle/input/multi-class-weather-dataset'))","metadata":{"id":"2UIA0qTn68lN","execution":{"iopub.status.busy":"2024-03-20T15:50:06.599090Z","iopub.execute_input":"2024-03-20T15:50:06.599603Z","iopub.status.idle":"2024-03-20T15:50:06.641408Z","shell.execute_reply.started":"2024-03-20T15:50:06.599570Z","shell.execute_reply":"2024-03-20T15:50:06.640438Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"data = []\nlabels = []\nlabel_names = []\nfor image_path in image_paths:\n    label = image_path.split(os.path.sep)[-2]\n    if label == 'BACKGROUND_Google':\n        continue\n\n    image = cv2.imread(image_path)\n     # Check if the image was loaded successfully\n    if image is None:\n        print(f\"Error loading image: {image_path}\")\n        continue\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    image = cv2.resize(image, (224, 224))\n\n    data.append(image)\n    label_names.append(label)\n    labels.append(label)\n","metadata":{"id":"8MywKplt7PFa","execution":{"iopub.status.busy":"2024-03-20T15:53:31.270368Z","iopub.execute_input":"2024-03-20T15:53:31.270914Z","iopub.status.idle":"2024-03-20T15:53:39.917819Z","shell.execute_reply.started":"2024-03-20T15:53:31.270875Z","shell.execute_reply":"2024-03-20T15:53:39.916484Z"},"trusted":true},"execution_count":123,"outputs":[{"name":"stdout","text":"Error loading image: /kaggle/input/multi-class-weather-dataset/Multi-class Weather Dataset/Rain/rain141.jpg\nError loading image: /kaggle/input/multi-class-weather-dataset/Multi-class Weather Dataset/Shine/shine131.jpg\n","output_type":"stream"}]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)            ","metadata":{"execution":{"iopub.status.busy":"2024-03-20T15:53:39.922217Z","iopub.execute_input":"2024-03-20T15:53:39.923013Z","iopub.status.idle":"2024-03-20T15:53:39.929352Z","shell.execute_reply.started":"2024-03-20T15:53:39.922974Z","shell.execute_reply":"2024-03-20T15:53:39.927743Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"data = np.array(data)\nlabels = np.array(labels)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dsTLCprK7MbP","outputId":"23b56b3c-5ab8-4f7c-db02-5382c4235169","execution":{"iopub.status.busy":"2024-03-20T15:53:39.931445Z","iopub.execute_input":"2024-03-20T15:53:39.932025Z","iopub.status.idle":"2024-03-20T15:53:40.031944Z","shell.execute_reply.started":"2024-03-20T15:53:39.931984Z","shell.execute_reply":"2024-03-20T15:53:40.030729Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"markdown","source":"Labels are in string format and we need to get it to number format ","metadata":{"id":"w4BT2IYFEuDi"}},{"cell_type":"code","source":"lb = LabelBinarizer()\nlabels = lb.fit_transform(labels)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ZmcyiLlEsXi","outputId":"d74aaff2-e159-4885-d6f0-a5a1aa773a09","execution":{"iopub.status.busy":"2024-03-20T15:53:40.034627Z","iopub.execute_input":"2024-03-20T15:53:40.035137Z","iopub.status.idle":"2024-03-20T15:53:40.044716Z","shell.execute_reply.started":"2024-03-20T15:53:40.035094Z","shell.execute_reply":"2024-03-20T15:53:40.043493Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"(X, X_Validation , Y, Y_Validation) = train_test_split(data, labels, \n                                                    test_size=0.2,  \n                                                    stratify=labels,\n                                                    random_state=42)\n\n(x_train, x_test, y_train, y_test) = train_test_split(X, Y, \n                                                    test_size=0.25, \n                                                    random_state=42)\n\nprint(f\"x_train examples: {x_train.shape}\\nx_test examples: {x_test.shape}\\nX_Validation examples: {X_Validation.shape}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jzjD1NvQ7uPl","outputId":"263544eb-1116-4b5e-c9ea-09d566bdded5","execution":{"iopub.status.busy":"2024-03-20T15:53:42.786495Z","iopub.execute_input":"2024-03-20T15:53:42.786971Z","iopub.status.idle":"2024-03-20T15:53:42.970694Z","shell.execute_reply.started":"2024-03-20T15:53:42.786937Z","shell.execute_reply":"2024-03-20T15:53:42.969372Z"},"trusted":true},"execution_count":127,"outputs":[{"name":"stdout","text":"x_train examples: (673, 224, 224, 3)\nx_test examples: (225, 224, 224, 3)\nX_Validation examples: (225, 224, 224, 3)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We need to do transformations on the images from this dataset as the model we are intending to use is the AlexNet model trained on ImageNet where the image is of size 224x224. We rescale the image as well as noramise it based on imagenet standards. Other than that since images per class is too low we also try some data augmentation on the train set explictly ","metadata":{"id":"lTGmm3Y0iBd2"}},{"cell_type":"code","source":"train_transform = transforms.Compose(\n    [transforms.ToPILImage(),\n\t transforms.Resize((224, 224)),\n     transforms.RandomRotation((-30, 30)),\n     transforms.RandomHorizontalFlip(p=0.5),\n     transforms.RandomVerticalFlip(p=0.5),\n     transforms.ToTensor(),\n     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                          std=[0.229, 0.224, 0.225])])\nval_transform = transforms.Compose(\n    [transforms.ToPILImage(),\n\t transforms.Resize((224, 224)),\n     transforms.ToTensor(),\n     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                          std=[0.229, 0.224, 0.225])])","metadata":{"id":"a_e9BW0WAKFk","execution":{"iopub.status.busy":"2024-03-20T15:53:45.919607Z","iopub.execute_input":"2024-03-20T15:53:45.920134Z","iopub.status.idle":"2024-03-20T15:53:45.930766Z","shell.execute_reply.started":"2024-03-20T15:53:45.920099Z","shell.execute_reply":"2024-03-20T15:53:45.929194Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"markdown","source":"Custom Dataset class which allows us to apply transformations and create a dataset object to pass into dataloader ","metadata":{"id":"gydwNaKZAS79"}},{"cell_type":"code","source":"# from torchvision.datasets import VisionDataset\n# from PIL import Image\n\n# class ImageDataset(VisionDataset):\n#     def __init__(self, images, labels=None, transforms=None, loader=None):\n#         super().__init__(None, transforms, None, None)\n#         self.images = images\n#         self.labels = labels\n#         self.transforms = transforms\n#         self.loader = loader  # Optional: specify a custom image loading function\n        \n#     def __len__(self):\n#         return len(self.images)\n    \n#     def __getitem__(self, index):\n#         img = self.loader(self.images[index]) if self.loader else self.default_loader(self.images[index])\n#         label = self.labels[index] if self.labels is not None else None\n\n#         if self.transforms:\n#             img = self.transforms(img)\n\n#         if label is not None:\n#             return img, label\n#         else:\n#             return img\n\n#     def default_loader(self, path):\n#         return Image.open(path).convert('RGB')\n\n\n\nfrom torchvision.datasets import VisionDataset\nfrom PIL import Image\nimport numpy as np\nfrom torchvision.transforms import ToPILImage\n\nclass ImageDataset(VisionDataset):\n    def __init__(self, images, labels=None, transforms=None, loader=None):\n        super().__init__(None, transforms, None, None)\n        self.images = images\n        self.labels = labels\n        self.transforms = transforms\n        self.loader = loader  # Optional: specify a custom image loading function\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, index):\n        img = self.images[index]\n        \n        # Check if img is a file path or a NumPy array\n        if isinstance(img, str):  # Check if img is a file path\n            img = self.loader(img) if self.loader else self.default_loader(img)\n        elif isinstance(img, np.ndarray):  # Check if img is a NumPy array\n            img = Image.fromarray(img)\n        else:\n            raise TypeError(\"Unsupported image type\")\n\n        label = self.labels[index] if self.labels is not None else None\n\n        # Skip ToPILImage transform if img is already a PIL Image\n        if isinstance(img, Image.Image):\n            if self.transforms:\n                for transform in self.transforms.transforms:\n                    if not isinstance(transform, ToPILImage):\n                        img = transform(img)\n        else:\n            if self.transforms:\n                img = self.transforms(img)\n\n        if label is not None:\n            return img, label\n        else:\n            return img\n\n    def default_loader(self, path):\n        return Image.open(path).convert('RGB')\n\n\n","metadata":{"id":"Yq2UvovOAOAM","execution":{"iopub.status.busy":"2024-03-20T15:57:18.396618Z","iopub.execute_input":"2024-03-20T15:57:18.397128Z","iopub.status.idle":"2024-03-20T15:57:18.413967Z","shell.execute_reply.started":"2024-03-20T15:57:18.397092Z","shell.execute_reply":"2024-03-20T15:57:18.412511Z"},"trusted":true},"execution_count":158,"outputs":[]},{"cell_type":"code","source":"train_data = ImageDataset(x_train, y_train, train_transform)\nval_data = ImageDataset(X_Validation, Y_Validation, val_transform)\ntest_data = ImageDataset(x_test, y_test, val_transform)\n \n# dataloaders\ntrainloader = DataLoader(train_data, batch_size=16, shuffle=True)\nvalloader = DataLoader(val_data, batch_size=16, shuffle=True)\ntestloader = DataLoader(test_data, batch_size=16, shuffle=False)","metadata":{"id":"AN0k6jR5ARhG","execution":{"iopub.status.busy":"2024-03-20T15:57:18.675808Z","iopub.execute_input":"2024-03-20T15:57:18.676266Z","iopub.status.idle":"2024-03-20T15:57:18.684857Z","shell.execute_reply.started":"2024-03-20T15:57:18.676234Z","shell.execute_reply":"2024-03-20T15:57:18.683112Z"},"trusted":true},"execution_count":159,"outputs":[]},{"cell_type":"markdown","source":"### Pre-trained Model Alexnet ","metadata":{"id":"ZXqGNYaSDLgf"}},{"cell_type":"code","source":"model = models.alexnet(pretrained=True)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TkdJX31YCIst","outputId":"7ecdb6de-d3ab-41bf-9382-74585cfec393","execution":{"iopub.status.busy":"2024-03-20T15:57:19.215992Z","iopub.execute_input":"2024-03-20T15:57:19.216500Z","iopub.status.idle":"2024-03-20T15:57:20.218339Z","shell.execute_reply.started":"2024-03-20T15:57:19.216466Z","shell.execute_reply":"2024-03-20T15:57:20.217028Z"},"trusted":true},"execution_count":160,"outputs":[]},{"cell_type":"code","source":"print(model)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AtLQUvBUEbmf","outputId":"c7b45016-381c-4a7c-e805-aaa1140f56e6","execution":{"iopub.status.busy":"2024-03-20T15:57:20.220561Z","iopub.execute_input":"2024-03-20T15:57:20.220964Z","iopub.status.idle":"2024-03-20T15:57:20.227453Z","shell.execute_reply.started":"2024-03-20T15:57:20.220932Z","shell.execute_reply":"2024-03-20T15:57:20.226082Z"},"trusted":true},"execution_count":161,"outputs":[{"name":"stdout","text":"AlexNet(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n  (classifier): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Linear(in_features=9216, out_features=4096, bias=True)\n    (2): ReLU(inplace=True)\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=4096, out_features=4096, bias=True)\n    (5): ReLU(inplace=True)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"for param in model.parameters():\n    param.requires_grad = False","metadata":{"id":"HaZDQLVief7B","execution":{"iopub.status.busy":"2024-03-20T15:57:20.229041Z","iopub.execute_input":"2024-03-20T15:57:20.229712Z","iopub.status.idle":"2024-03-20T15:57:20.243624Z","shell.execute_reply.started":"2024-03-20T15:57:20.229676Z","shell.execute_reply":"2024-03-20T15:57:20.242240Z"},"trusted":true},"execution_count":162,"outputs":[]},{"cell_type":"code","source":"model.classifier[6] = nn.Linear(in_features=4096,out_features=101,bias=True)","metadata":{"id":"-lui0edrLRQk","execution":{"iopub.status.busy":"2024-03-20T15:57:20.249473Z","iopub.execute_input":"2024-03-20T15:57:20.249936Z","iopub.status.idle":"2024-03-20T15:57:20.264814Z","shell.execute_reply.started":"2024-03-20T15:57:20.249902Z","shell.execute_reply":"2024-03-20T15:57:20.263010Z"},"trusted":true},"execution_count":163,"outputs":[]},{"cell_type":"code","source":"for param in model.classifier[6].parameters():\n  param.requires_grad = True","metadata":{"id":"SiEKu4GgfxRb","execution":{"iopub.status.busy":"2024-03-20T15:57:20.266659Z","iopub.execute_input":"2024-03-20T15:57:20.267345Z","iopub.status.idle":"2024-03-20T15:57:20.275531Z","shell.execute_reply.started":"2024-03-20T15:57:20.267293Z","shell.execute_reply":"2024-03-20T15:57:20.274077Z"},"trusted":true},"execution_count":164,"outputs":[]},{"cell_type":"code","source":"model ","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NoaKJtk3NGb3","outputId":"7ce3a49a-b227-43bb-a944-7d4a11a0decf","execution":{"iopub.status.busy":"2024-03-20T15:57:20.277008Z","iopub.execute_input":"2024-03-20T15:57:20.277425Z","iopub.status.idle":"2024-03-20T15:57:20.290735Z","shell.execute_reply.started":"2024-03-20T15:57:20.277380Z","shell.execute_reply":"2024-03-20T15:57:20.289476Z"},"trusted":true},"execution_count":165,"outputs":[{"execution_count":165,"output_type":"execute_result","data":{"text/plain":"AlexNet(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n  (classifier): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Linear(in_features=9216, out_features=4096, bias=True)\n    (2): ReLU(inplace=True)\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=4096, out_features=4096, bias=True)\n    (5): ReLU(inplace=True)\n    (6): Linear(in_features=4096, out_features=101, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"model.to(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_0ZKg3_7FeFj","outputId":"f4962d1f-576f-4b22-93ea-6bdb8aafacb1","execution":{"iopub.status.busy":"2024-03-20T15:57:20.293659Z","iopub.execute_input":"2024-03-20T15:57:20.294377Z","iopub.status.idle":"2024-03-20T15:57:20.307453Z","shell.execute_reply.started":"2024-03-20T15:57:20.294330Z","shell.execute_reply":"2024-03-20T15:57:20.305873Z"},"trusted":true},"execution_count":166,"outputs":[{"execution_count":166,"output_type":"execute_result","data":{"text/plain":"AlexNet(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n  (classifier): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Linear(in_features=9216, out_features=4096, bias=True)\n    (2): ReLU(inplace=True)\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=4096, out_features=4096, bias=True)\n    (5): ReLU(inplace=True)\n    (6): Linear(in_features=4096, out_features=101, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.01)\n# Define your loss function\ncriterion = nn.CrossEntropyLoss()","metadata":{"id":"dDFlmgpVD5S_","execution":{"iopub.status.busy":"2024-03-20T15:57:20.309184Z","iopub.execute_input":"2024-03-20T15:57:20.309988Z","iopub.status.idle":"2024-03-20T15:57:20.317743Z","shell.execute_reply.started":"2024-03-20T15:57:20.309947Z","shell.execute_reply":"2024-03-20T15:57:20.316481Z"},"trusted":true},"execution_count":167,"outputs":[]},{"cell_type":"code","source":"epochs = 10 ","metadata":{"id":"6Sw_LklqEp8z","execution":{"iopub.status.busy":"2024-03-20T15:57:20.319382Z","iopub.execute_input":"2024-03-20T15:57:20.319822Z","iopub.status.idle":"2024-03-20T15:57:20.331001Z","shell.execute_reply.started":"2024-03-20T15:57:20.319787Z","shell.execute_reply":"2024-03-20T15:57:20.329721Z"},"trusted":true},"execution_count":168,"outputs":[]},{"cell_type":"markdown","source":"#### Training and Validation ","metadata":{"id":"Bh6WXYhABNbR"}},{"cell_type":"code","source":"def fit(model, dataloader):\n    print('Training')\n    model.train()\n    Train_loss = 0.0\n    train_running_correct = 0\n    for i, data in tqdm(enumerate(dataloader), total=int(len(train_data)/dataloader.batch_size)):\n        data, target = data[0].to(device), data[1].to(device)\n        optimizer.zero_grad()\n        outputs = model(data)\n        loss = criterion(outputs, torch.max(target, 1)[1])\n        Train_loss += loss.item()\n        _, prediction = torch.max(outputs.data, 1)\n        train_running_correct += (prediction == torch.max(target, 1)[1]).sum().item()\n        loss.backward()\n        optimizer.step()\n        \n    Train_loss = Train_loss/len(dataloader.dataset)\n    Train_acc = 100. * train_running_correct/len(dataloader.dataset)\n    \n    print(f\"Train Loss: {Train_loss:.4f}, Train Acc: {Train_acc:.2f}\")\n    \n    return Train_loss, Train_acc","metadata":{"id":"EcY6sb7BBeYb","execution":{"iopub.status.busy":"2024-03-20T15:57:20.390486Z","iopub.execute_input":"2024-03-20T15:57:20.390984Z","iopub.status.idle":"2024-03-20T15:57:20.402617Z","shell.execute_reply.started":"2024-03-20T15:57:20.390949Z","shell.execute_reply":"2024-03-20T15:57:20.401083Z"},"trusted":true},"execution_count":169,"outputs":[]},{"cell_type":"code","source":"#validation function\ndef validate(model, dataloader):\n    print('Validating')\n    model.eval()\n    val_running_loss = 0.0\n    val_running_correct = 0\n    with torch.no_grad():\n        for i, data in tqdm(enumerate(dataloader), total=int(len(val_data)/dataloader.batch_size)):\n            data, target = data[0].to(device), data[1].to(device)\n            outputs = model(data)\n            loss = criterion(outputs, torch.max(target, 1)[1])\n            \n            val_running_loss += loss.item()\n            _, prediction = torch.max(outputs.data, 1)\n            val_running_correct += (prediction == torch.max(target, 1)[1]).sum().item()\n        \n        Validation_loss = val_running_loss/len(dataloader.dataset)\n        Validation_acc = 100. * val_running_correct/len(dataloader.dataset)\n        print(f'Val Loss: {Validation_loss:.4f}, Val Acc: {Validation_acc:.2f}')\n        \n        return Validation_loss, Validation_acc","metadata":{"id":"nXjROMH4Bby1","execution":{"iopub.status.busy":"2024-03-20T15:57:20.914944Z","iopub.execute_input":"2024-03-20T15:57:20.915460Z","iopub.status.idle":"2024-03-20T15:57:20.926994Z","shell.execute_reply.started":"2024-03-20T15:57:20.915424Z","shell.execute_reply":"2024-03-20T15:57:20.925479Z"},"trusted":true},"execution_count":170,"outputs":[]},{"cell_type":"code","source":"print(f\"Training on {len(train_data)} examples, validating on {len(val_data)} examples\")\nTrain_loss , Train_acc = [], []\nValidation_loss , Validation_acc = [], []\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch+1} of {epochs}\")\n    Validation_epoch_loss, Validation_epoch_acc = validate(model, valloader)\n    train_epoch_loss, train_epoch_accuracy = fit(model, trainloader)\n    Validation_loss.append(Validation_epoch_loss)\n    Validation_acc.append(Validation_epoch_acc)\n    Train_acc.append(train_epoch_accuracy)\n    Train_loss.append(train_epoch_loss)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8IilTIRUBQDL","outputId":"d5640865-858c-4462-b1a0-89aea1d29228","execution":{"iopub.status.busy":"2024-03-20T15:57:20.939655Z","iopub.execute_input":"2024-03-20T15:57:20.940140Z","iopub.status.idle":"2024-03-20T16:00:28.245137Z","shell.execute_reply.started":"2024-03-20T15:57:20.940108Z","shell.execute_reply":"2024-03-20T16:00:28.243722Z"},"trusted":true},"execution_count":171,"outputs":[{"name":"stdout","text":"Training on 673 examples, validating on 225 examples\nEpoch 1 of 10\nValidating\n","output_type":"stream"},{"name":"stderr","text":"15it [00:04,  3.25it/s]                        \n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.3053, Val Acc: 4.00\nTraining\n","output_type":"stream"},{"name":"stderr","text":"43it [00:14,  2.95it/s]                        \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0574, Train Acc: 85.74\nEpoch 2 of 10\nValidating\n","output_type":"stream"},{"name":"stderr","text":"15it [00:04,  3.47it/s]                        \n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0653, Val Acc: 93.78\nTraining\n","output_type":"stream"},{"name":"stderr","text":"43it [00:13,  3.09it/s]                        \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0394, Train Acc: 90.64\nEpoch 3 of 10\nValidating\n","output_type":"stream"},{"name":"stderr","text":"15it [00:05,  2.95it/s]                        \n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0457, Val Acc: 91.56\nTraining\n","output_type":"stream"},{"name":"stderr","text":"43it [00:13,  3.11it/s]                        \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0413, Train Acc: 92.87\nEpoch 4 of 10\nValidating\n","output_type":"stream"},{"name":"stderr","text":"15it [00:04,  3.27it/s]                        \n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0339, Val Acc: 94.67\nTraining\n","output_type":"stream"},{"name":"stderr","text":"43it [00:14,  2.87it/s]                        \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0382, Train Acc: 92.27\nEpoch 5 of 10\nValidating\n","output_type":"stream"},{"name":"stderr","text":"15it [00:04,  3.44it/s]                        \n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0379, Val Acc: 95.56\nTraining\n","output_type":"stream"},{"name":"stderr","text":"43it [00:13,  3.08it/s]                        \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0427, Train Acc: 92.57\nEpoch 6 of 10\nValidating\n","output_type":"stream"},{"name":"stderr","text":"15it [00:04,  3.43it/s]                        \n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0291, Val Acc: 96.00\nTraining\n","output_type":"stream"},{"name":"stderr","text":"43it [00:14,  2.97it/s]                        \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0432, Train Acc: 93.31\nEpoch 7 of 10\nValidating\n","output_type":"stream"},{"name":"stderr","text":"15it [00:04,  3.43it/s]                        \n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0339, Val Acc: 94.22\nTraining\n","output_type":"stream"},{"name":"stderr","text":"43it [00:13,  3.12it/s]                        \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0402, Train Acc: 94.06\nEpoch 8 of 10\nValidating\n","output_type":"stream"},{"name":"stderr","text":"15it [00:04,  3.07it/s]                        \n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0379, Val Acc: 96.89\nTraining\n","output_type":"stream"},{"name":"stderr","text":"43it [00:13,  3.13it/s]                        \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0427, Train Acc: 93.76\nEpoch 9 of 10\nValidating\n","output_type":"stream"},{"name":"stderr","text":"15it [00:04,  3.47it/s]                        \n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0836, Val Acc: 90.67\nTraining\n","output_type":"stream"},{"name":"stderr","text":"43it [00:14,  2.95it/s]                        \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0585, Train Acc: 93.61\nEpoch 10 of 10\nValidating\n","output_type":"stream"},{"name":"stderr","text":"15it [00:04,  3.45it/s]                        \n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0987, Val Acc: 87.11\nTraining\n","output_type":"stream"},{"name":"stderr","text":"43it [00:14,  3.06it/s]                        ","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0521, Train Acc: 92.27\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Test","metadata":{"id":"TTjOA2ucG8iP"}},{"cell_type":"code","source":"correct = 0\ntotal = 0\nwith torch.no_grad():\n    for data in testloader:\n        inputs, target = data[0].to(device, non_blocking=True), data[1].to(device, non_blocking=True)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs.data, 1)\n        total += target.size(0)\n        correct += (predicted == torch.max(target, 1)[1]).sum().item()\n\nprint('Accuracy of the network on test images: %0.3f %%' % (\n    100 * correct / total))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H7Z2IlnaG9XW","outputId":"655bcd8f-3afc-4b1b-c77e-17d7402ffd88","execution":{"iopub.status.busy":"2024-03-20T16:00:28.247686Z","iopub.execute_input":"2024-03-20T16:00:28.248499Z","iopub.status.idle":"2024-03-20T16:00:32.608264Z","shell.execute_reply.started":"2024-03-20T16:00:28.248458Z","shell.execute_reply":"2024-03-20T16:00:32.606596Z"},"trusted":true},"execution_count":172,"outputs":[{"name":"stdout","text":"Accuracy of the network on test images: 90.667 %\n","output_type":"stream"}]}]}